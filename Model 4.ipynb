{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54307,"status":"ok","timestamp":1732312628313,"user":{"displayName":"Amalu Tomy","userId":"02293207622035948550"},"user_tz":-630},"id":"HmV23RaaJvHA","outputId":"4d0ad261-4ad5-4219-d529-8f491dc4824a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import pandas as pd\n","# Load the dataset\n","downsample_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Final project/Model 2/downsample_df.csv')"],"metadata":{"id":"uMEJ1HYn38Lh","executionInfo":{"status":"ok","timestamp":1732312636961,"user_tz":-630,"elapsed":2102,"user":{"displayName":"Amalu Tomy","userId":"02293207622035948550"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_squared_error\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout\n","\n","# Load dataset\n","downsample_data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Final project/Model 2/downsample_df.csv')\n","\n","# Split features and targets\n","X = downsample_data.drop(columns=['RFL_Att', 'FSO_Att'])  # Features\n","y_rfl = downsample_data['RFL_Att']  # Target 1\n","y_fso = downsample_data['FSO_Att']  # Target 2\n","\n","# Split into training and testing sets\n","X_train, X_test, y_rfl_train, y_rfl_test, y_fso_train, y_fso_test = train_test_split(\n","    X, y_rfl, y_fso, test_size=0.2, random_state=42\n",")\n","\n","# Standardize the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Build the neural network\n","model = Sequential([\n","    Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n","    Dropout(0.2),\n","    Dense(64, activation='relu'),\n","    Dropout(0.2),\n","    Dense(32, activation='relu'),\n","    Dense(2)  # Output layer for two predictions (RFL_Att and FSO_Att)\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n","\n","# Prepare targets for training (combine into a single array)\n","y_train_combined = np.column_stack((y_rfl_train, y_fso_train))\n","y_test_combined = np.column_stack((y_rfl_test, y_fso_test))\n","\n","# Train the model\n","history = model.fit(X_train_scaled, y_train_combined, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n","\n","# Evaluate the model\n","y_pred_combined = model.predict(X_test_scaled)\n","\n","# Separate predictions for RFL_Att and FSO_Att\n","y_rfl_pred = y_pred_combined[:, 0]\n","y_fso_pred = y_pred_combined[:, 1]\n","\n","# Calculate RMSE for RFL_Att and FSO_Att\n","rmse_rfl = np.sqrt(mean_squared_error(y_rfl_test, y_rfl_pred))\n","rmse_fso = np.sqrt(mean_squared_error(y_fso_test, y_fso_pred))\n","\n","# Print RMSE values\n","print(f\"RMSE for RFL_Att: {rmse_rfl:.4f}\")\n","print(f\"RMSE for FSO_Att: {rmse_fso:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5_f_rtTt4SmS","executionInfo":{"status":"ok","timestamp":1732312943620,"user_tz":-630,"elapsed":74315,"user":{"displayName":"Amalu Tomy","userId":"02293207622035948550"}},"outputId":"cca8fa83-28fb-45e7-e743-f02335eb0ae8"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - loss: 30.6117 - mse: 30.6117 - val_loss: 7.6932 - val_mse: 7.6932\n","Epoch 2/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 8.9836 - mse: 8.9836 - val_loss: 5.5399 - val_mse: 5.5399\n","Epoch 3/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.1251 - mse: 7.1251 - val_loss: 4.4718 - val_mse: 4.4718\n","Epoch 4/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 6.2304 - mse: 6.2304 - val_loss: 3.7678 - val_mse: 3.7678\n","Epoch 5/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6251 - mse: 5.6251 - val_loss: 4.3832 - val_mse: 4.3832\n","Epoch 6/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.2481 - mse: 5.2481 - val_loss: 3.6981 - val_mse: 3.6981\n","Epoch 7/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.7765 - mse: 4.7765 - val_loss: 3.1553 - val_mse: 3.1553\n","Epoch 8/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.5519 - mse: 4.5519 - val_loss: 3.4981 - val_mse: 3.4981\n","Epoch 9/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.2555 - mse: 4.2555 - val_loss: 3.5955 - val_mse: 3.5955\n","Epoch 10/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0843 - mse: 4.0843 - val_loss: 2.7702 - val_mse: 2.7702\n","Epoch 11/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.8832 - mse: 3.8832 - val_loss: 2.7247 - val_mse: 2.7247\n","Epoch 12/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 3.7106 - mse: 3.7106 - val_loss: 3.2229 - val_mse: 3.2229\n","Epoch 13/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 3.5331 - mse: 3.5331 - val_loss: 2.9854 - val_mse: 2.9854\n","Epoch 14/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6024 - mse: 3.6024 - val_loss: 3.2612 - val_mse: 3.2612\n","Epoch 15/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4368 - mse: 3.4368 - val_loss: 2.7061 - val_mse: 2.7061\n","Epoch 16/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3083 - mse: 3.3083 - val_loss: 2.8915 - val_mse: 2.8915\n","Epoch 17/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.1109 - mse: 3.1109 - val_loss: 2.7348 - val_mse: 2.7348\n","Epoch 18/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.2003 - mse: 3.2003 - val_loss: 2.4098 - val_mse: 2.4098\n","Epoch 19/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.1040 - mse: 3.1040 - val_loss: 2.7831 - val_mse: 2.7831\n","Epoch 20/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3898 - mse: 3.3898 - val_loss: 2.6882 - val_mse: 2.6882\n","Epoch 21/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.9503 - mse: 2.9503 - val_loss: 2.6001 - val_mse: 2.6001\n","Epoch 22/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.8323 - mse: 2.8323 - val_loss: 2.6747 - val_mse: 2.6747\n","Epoch 23/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.8046 - mse: 2.8046 - val_loss: 2.3780 - val_mse: 2.3780\n","Epoch 24/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.7675 - mse: 2.7675 - val_loss: 2.2540 - val_mse: 2.2540\n","Epoch 25/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7425 - mse: 2.7425 - val_loss: 2.4730 - val_mse: 2.4730\n","Epoch 26/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7677 - mse: 2.7677 - val_loss: 2.3079 - val_mse: 2.3079\n","Epoch 27/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7395 - mse: 2.7395 - val_loss: 2.1017 - val_mse: 2.1017\n","Epoch 28/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7535 - mse: 2.7535 - val_loss: 2.1785 - val_mse: 2.1785\n","Epoch 29/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5630 - mse: 2.5630 - val_loss: 2.3975 - val_mse: 2.3975\n","Epoch 30/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.7217 - mse: 2.7217 - val_loss: 1.9364 - val_mse: 1.9364\n","Epoch 31/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5192 - mse: 2.5192 - val_loss: 2.1163 - val_mse: 2.1163\n","Epoch 32/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.5870 - mse: 2.5870 - val_loss: 1.9618 - val_mse: 1.9618\n","Epoch 33/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.5530 - mse: 2.5530 - val_loss: 2.3312 - val_mse: 2.3312\n","Epoch 34/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - loss: 2.6029 - mse: 2.6029 - val_loss: 2.2438 - val_mse: 2.2438\n","Epoch 35/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.3943 - mse: 2.3943 - val_loss: 1.8823 - val_mse: 1.8823\n","Epoch 36/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4742 - mse: 2.4742 - val_loss: 2.2903 - val_mse: 2.2903\n","Epoch 37/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3238 - mse: 2.3238 - val_loss: 2.6863 - val_mse: 2.6863\n","Epoch 38/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.5898 - mse: 2.5898 - val_loss: 2.0182 - val_mse: 2.0182\n","Epoch 39/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3938 - mse: 2.3938 - val_loss: 1.8441 - val_mse: 1.8441\n","Epoch 40/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.3864 - mse: 2.3864 - val_loss: 1.8685 - val_mse: 1.8685\n","Epoch 41/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.4801 - mse: 2.4801 - val_loss: 2.1249 - val_mse: 2.1249\n","Epoch 42/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.3833 - mse: 2.3833 - val_loss: 1.8371 - val_mse: 1.8371\n","Epoch 43/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 2.4211 - mse: 2.4211 - val_loss: 1.8661 - val_mse: 1.8661\n","Epoch 44/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.2229 - mse: 2.2229 - val_loss: 1.9231 - val_mse: 1.9231\n","Epoch 45/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 2.2239 - mse: 2.2239 - val_loss: 1.8912 - val_mse: 1.8912\n","Epoch 46/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 2.2229 - mse: 2.2229 - val_loss: 1.7522 - val_mse: 1.7522\n","Epoch 47/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2624 - mse: 2.2624 - val_loss: 1.7027 - val_mse: 1.7027\n","Epoch 48/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2344 - mse: 2.2344 - val_loss: 2.0734 - val_mse: 2.0734\n","Epoch 49/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2219 - mse: 2.2219 - val_loss: 1.7473 - val_mse: 1.7473\n","Epoch 50/50\n","\u001b[1m383/383\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.2548 - mse: 2.2548 - val_loss: 2.0054 - val_mse: 2.0054\n","\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n","RMSE for RFL_Att: 1.1829\n","RMSE for FSO_Att: 1.7919\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import r2_score\n","\n","# Evaluate the model\n","y_pred_combined = model.predict(X_test_scaled)\n","\n","# Separate predictions for RFL_Att and FSO_Att\n","y_rfl_pred = y_pred_combined[:, 0]\n","y_fso_pred = y_pred_combined[:, 1]\n","\n","# Calculate RMSE for RFL_Att and FSO_Att\n","rmse_rfl = np.sqrt(mean_squared_error(y_rfl_test, y_rfl_pred))\n","rmse_fso = np.sqrt(mean_squared_error(y_fso_test, y_fso_pred))\n","\n","# Calculate R² for RFL_Att and FSO_Att\n","r2_rfl = r2_score(y_rfl_test, y_rfl_pred)\n","r2_fso = r2_score(y_fso_test, y_fso_pred)\n","\n","# Print RMSE and R² values\n","print(f\"RMSE for RFL_Att: {rmse_rfl:.4f}\")\n","print(f\"R² for RFL_Att: {r2_rfl:.4f}\")\n","print(f\"RMSE for FSO_Att: {rmse_fso:.4f}\")\n","print(f\"R² for FSO_Att: {r2_fso:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xv6XzQSl5gvU","executionInfo":{"status":"ok","timestamp":1732312969839,"user_tz":-630,"elapsed":955,"user":{"displayName":"Amalu Tomy","userId":"02293207622035948550"}},"outputId":"274dcf2a-25a6-48ed-9eac-65089c9d3190"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m120/120\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n","RMSE for RFL_Att: 1.1829\n","R² for RFL_Att: 0.9081\n","RMSE for FSO_Att: 1.7919\n","R² for FSO_Att: 0.8323\n"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOnkxp5inje64LBFJSZPRcO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}